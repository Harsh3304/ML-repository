{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11225716,"sourceType":"datasetVersion","datasetId":7011247}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport time\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torchvision.utils\nfrom tqdm import tqdm  # progress bar\n\n# ---------------------------\n# Parameters (hard-coded)\n# ---------------------------\nINPUT_DIR = \"/kaggle/input/processed-faces/processed_faces\"\n\nBATCH_SIZE = 22\nEPOCHS = 26\nLR = 0.0002\nCHECKPOINT_DIR = \"checkpoints\"\n# Optional inference parameters:\nTEST_IMAGE = \"\"  # Set to a valid image path for testing (e.g., \"sample.jpg\"), or leave as empty string\nTIME_PARAM = 0.75\nOUTPUT_IMAGE = \"123.jpg\"\n\n# =============================================================================\n# CUSTOM DATASET\n# =============================================================================\nclass TimeVariableDeepfakesDataset(Dataset):\n    \"\"\"\n    Custom Dataset for Time-Variable Deepfakes.\n    \n    Assumes that image filenames contain an age label (e.g., \"23_image.jpg\").\n    The age is normalized to a continuous time parameter between 0 and 1.\n    \"\"\"\n    def __init__(self, root_dir, transform=None, min_age=0, max_age=100):\n        self.root_dir = root_dir\n        self.image_paths = glob.glob(os.path.join(root_dir, '*.jpg'))\n        self.transform = transform\n        self.min_age = min_age\n        self.max_age = max_age\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def _extract_age(self, filename):\n        base = os.path.basename(filename)\n        age_str = base.split('_')[0]\n        try:\n            age = int(age_str)\n        except ValueError:\n            age = self.min_age\n        return age\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        age = self._extract_age(img_path)\n        # Normalize age to [0, 1]\n        time_param = (age - self.min_age) / (self.max_age - self.min_age)\n        time_param = torch.tensor([time_param], dtype=torch.float32)\n        return image, time_param\n\n# =============================================================================\n# MODEL DEFINITIONS\n# =============================================================================\nclass SelfAttention(nn.Module):\n    \"\"\"\n    Self-Attention Block as used in SAGAN.\n    \"\"\"\n    def __init__(self, in_dim):\n        super(SelfAttention, self).__init__()\n        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n        self.key_conv   = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax  = nn.Softmax(dim=-1)\n    \n    def forward(self, x):\n        B, C, W, H = x.size()\n        proj_query = self.query_conv(x).view(B, -1, W * H).permute(0, 2, 1)\n        proj_key = self.key_conv(x).view(B, -1, W * H)\n        energy = torch.bmm(proj_query, proj_key)\n        attention = self.softmax(energy)\n        proj_value = self.value_conv(x).view(B, -1, W * H)\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n        out = out.view(B, C, W, H)\n        out = self.gamma * out + x\n        return out\n\nclass UNetGenerator(nn.Module):\n    \"\"\"\n    U-Net style Generator with attention blocks.\n    Conditions generation on a continuous time variable.\n    \n    The time parameter is embedded and concatenated to the input image.\n    \"\"\"\n    def __init__(self, in_channels=3, out_channels=3, time_dim=1, feature_channels=64):\n        super(UNetGenerator, self).__init__()\n        self.feature_channels = feature_channels\n        \n        # Time embedding: embeds the time parameter into a vector of size feature_channels.\n        self.time_embedding = nn.Sequential(\n            nn.Linear(time_dim, feature_channels),\n            nn.ReLU(True),\n            nn.Linear(feature_channels, feature_channels)\n        )\n        # Update encoder to accept extra channels from time embedding.\n        self.enc1 = self.conv_block(in_channels + feature_channels, feature_channels)\n        self.enc2 = self.conv_block(feature_channels, feature_channels * 2)\n        self.enc3 = self.conv_block(feature_channels * 2, feature_channels * 4)\n        self.enc4 = self.conv_block(feature_channels * 4, feature_channels * 8)\n        \n        # Bottleneck with attention\n        self.bottleneck = self.conv_block(feature_channels * 8, feature_channels * 16)\n        self.attention = SelfAttention(feature_channels * 16)\n        \n        # Decoder with skip connections\n        self.dec4 = self.deconv_block(feature_channels * 16 + feature_channels * 8, feature_channels * 8)\n        self.dec3 = self.deconv_block(feature_channels * 8 + feature_channels * 4, feature_channels * 4)\n        self.dec2 = self.deconv_block(feature_channels * 4 + feature_channels * 2, feature_channels * 2)\n        self.dec1 = self.deconv_block(feature_channels * 2 + feature_channels, feature_channels)\n        \n        self.final_conv = nn.Conv2d(feature_channels, out_channels, kernel_size=1)\n        self.tanh = nn.Tanh()\n\n    def conv_block(self, in_channels, out_channels):\n        \"\"\"Convolutional block: Conv2d -> BatchNorm -> LeakyReLU.\"\"\"\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n    def deconv_block(self, in_channels, out_channels):\n        \"\"\"Deconvolutional block: ConvTranspose2d -> BatchNorm -> ReLU.\"\"\"\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x, time_param):\n        # Embed and spatially expand the time parameter.\n        t_emb = self.time_embedding(time_param)  # Shape: (B, feature_channels)\n        B, _, H, W = x.size()\n        t_emb_expanded = t_emb.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, H, W)\n        x = torch.cat([x, t_emb_expanded], dim=1)\n        \n        # Encoder\n        e1 = self.enc1(x)    # (B, feature_channels, H/2, W/2)\n        e2 = self.enc2(e1)   # (B, 2*feature_channels, H/4, W/4)\n        e3 = self.enc3(e2)   # (B, 4*feature_channels, H/8, W/8)\n        e4 = self.enc4(e3)   # (B, 8*feature_channels, H/16, W/16)\n        \n        # Bottleneck + Attention\n        b = self.bottleneck(e4)  # (B, 16*feature_channels, H/32, W/32)\n        b = self.attention(b)\n        \n        # Upsample bottleneck output to match e4 spatial size.\n        b = nn.functional.interpolate(b, scale_factor=2, mode='bilinear', align_corners=True)\n        \n        # Decoder with skip connections\n        d4 = self.dec4(torch.cat([b, e4], dim=1))\n        d3 = self.dec3(torch.cat([d4, e3], dim=1))\n        d2 = self.dec2(torch.cat([d3, e2], dim=1))\n        d1 = self.dec1(torch.cat([d2, e1], dim=1))\n        out = self.tanh(self.final_conv(d1))\n        return out\n\nclass MultiScaleDiscriminator(nn.Module):\n    \"\"\"\n    Multi-Scale Discriminator to enforce realism at different resolutions.\n    \"\"\"\n    def __init__(self, in_channels=3, feature_channels=64, num_scales=3):\n        super(MultiScaleDiscriminator, self).__init__()\n        self.num_scales = num_scales\n        self.discriminators = nn.ModuleList()\n        for _ in range(num_scales):\n            self.discriminators.append(self.make_discriminator(in_channels, feature_channels))\n        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n\n    def make_discriminator(self, in_channels, feature_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, feature_channels, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(feature_channels, feature_channels * 2, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(feature_channels * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(feature_channels * 2, feature_channels * 4, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(feature_channels * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(feature_channels * 4, 1, kernel_size=4, padding=1)\n        )\n\n    def forward(self, x):\n        outputs = []\n        for disc in self.discriminators:\n            out = disc(x)\n            outputs.append(out)\n            x = self.downsample(x)\n        return outputs\n\nclass VGGFeatureExtractor(nn.Module):\n    \"\"\"\n    VGG16 Feature Extractor for computing Perceptual Loss.\n    \"\"\"\n    def __init__(self):\n        super(VGGFeatureExtractor, self).__init__()\n        vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n        self.features = vgg16.features.eval()\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        return self.features(x)\n\n# =============================================================================\n# LOSS FUNCTIONS\n# =============================================================================\ndef adversarial_loss(outputs, target_is_real):\n    \"\"\"\n    Compute adversarial loss using Mean Squared Error.\n    \"\"\"\n    loss = 0\n    target_tensor = 1.0 if target_is_real else 0.0\n    criterion = nn.MSELoss()\n    for output in outputs:\n        loss += criterion(output, torch.full_like(output, target_tensor))\n    return loss\n\ndef identity_loss(input_image, generated_image):\n    \"\"\"\n    L1 loss to preserve the subject's identity.\n    \"\"\"\n    return nn.L1Loss()(generated_image, input_image)\n\ndef perceptual_loss(vgg, input_image, generated_image, layers=('8', '15')):\n    \"\"\"\n    Compute perceptual loss based on intermediate VGG features.\n    \n    The layer numbers (as strings) indicate which features to use.\n    \"\"\"\n    loss = 0.0\n    x = input_image\n    y = generated_image\n    for name, layer in vgg.features._modules.items():\n        x = layer(x)\n        y = layer(y)\n        if name in layers:\n            loss += nn.L1Loss()(x, y)\n    return loss\n\ndef time_consistency_loss(generator, input_image, time1, time2):\n    \"\"\"\n    Enforce smooth transitions: small changes in time should yield similar outputs.\n    \"\"\"\n    gen1 = generator(input_image, time1)\n    gen2 = generator(input_image, time2)\n    return nn.L1Loss()(gen1, gen2)\n\n# =============================================================================\n# TRAINING & TESTING FUNCTIONS\n# =============================================================================\ndef train(model_G, model_D, dataloader, optimizer_G, optimizer_D, vgg, device, num_epochs=20, checkpoint_dir='checkpoints'):\n    \"\"\"\n    Train the conditional GAN.\n    \"\"\"\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n    \n    for epoch in range(num_epochs):\n        epoch_start = time.time()\n        # Wrap dataloader with tqdm for live progress.\n        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n        for i, (real_images, time_params) in enumerate(progress_bar):\n            real_images = real_images.to(device)\n            time_params = time_params.to(device)\n            \n            # ---------------------\n            # Train Generator\n            # ---------------------\n            optimizer_G.zero_grad()\n            fake_images = model_G(real_images, time_params)\n            disc_outputs = model_D(fake_images)\n            loss_adv = adversarial_loss(disc_outputs, True)\n            loss_id = identity_loss(real_images, fake_images)\n            loss_perc = perceptual_loss(vgg, real_images, fake_images)\n            # Sample a nearby time parameter (delta=0.05, clamped to [0,1])\n            delta = 0.05\n            time_plus = torch.clamp(time_params + delta, 0, 1)\n            loss_time = time_consistency_loss(model_G, real_images, time_params, time_plus)\n            \n            loss_G = loss_adv + 10.0 * loss_id + 5.0 * loss_perc + 2.0 * loss_time\n            loss_G.backward()\n            optimizer_G.step()\n            \n            # ---------------------\n            # Train Discriminator\n            # ---------------------\n            optimizer_D.zero_grad()\n            real_outputs = model_D(real_images)\n            loss_real = adversarial_loss(real_outputs, True)\n            fake_outputs = model_D(fake_images.detach())\n            loss_fake = adversarial_loss(fake_outputs, False)\n            loss_D = (loss_real + loss_fake) * 0.5\n            loss_D.backward()\n            optimizer_D.step()\n            \n            # Update progress bar with current losses.\n            progress_bar.set_postfix({\n                \"Loss_G\": loss_G.item(),\n                \"Loss_D\": loss_D.item()\n            })\n                \n        # Save a checkpoint after each epoch.\n        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n        torch.save({\n            'epoch': epoch,\n            'generator_state_dict': model_G.state_dict(),\n            'discriminator_state_dict': model_D.state_dict(),\n            'optimizer_G_state_dict': optimizer_G.state_dict(),\n            'optimizer_D_state_dict': optimizer_D.state_dict()\n        }, checkpoint_path)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {time.time()-epoch_start:.2f} seconds. Checkpoint saved to {checkpoint_path}.\")\n\ndef test(generator, input_image, time_param, device, output_path='output.jpg'):\n    \"\"\"\n    Run inference: generate a deepfake image for a given time parameter.\n    \"\"\"\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n    ])\n    if isinstance(input_image, Image.Image):\n        input_tensor = transform(input_image).unsqueeze(0).to(device)\n    else:\n        input_tensor = input_image.to(device)\n    \n    time_tensor = torch.tensor([[time_param]], dtype=torch.float32).to(device)\n    generator.eval()\n    with torch.no_grad():\n        fake = generator(input_tensor, time_tensor)\n    fake = (fake + 1) / 2  # Denormalize from [-1,1] to [0,1]\n    torchvision.utils.save_image(fake, output_path)\n    print(f\"Generated image saved to {output_path}\")\n\n# =============================================================================\n# MAIN FUNCTION\n# =============================================================================\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Data transforms for training.\n    data_transforms = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n    ])\n    dataset = TimeVariableDeepfakesDataset(root_dir=INPUT_DIR, transform=data_transforms)\n    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n    \n    # Initialize models.\n    generator = UNetGenerator(in_channels=3, out_channels=3, time_dim=1, feature_channels=64).to(device)\n    discriminator = MultiScaleDiscriminator(in_channels=3, feature_channels=64).to(device)\n    vgg = VGGFeatureExtractor().to(device)\n    \n    optimizer_G = optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.999))\n    optimizer_D = optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.999))\n    \n    # Train the models.\n    train(generator, discriminator, dataloader, optimizer_G, optimizer_D, vgg, device,\n          num_epochs=EPOCHS, checkpoint_dir=CHECKPOINT_DIR)\n    \n    # Optional inference test.\n    if TEST_IMAGE:\n        sample_image = Image.open(TEST_IMAGE).convert('RGB')\n        test(generator, sample_image, time_param=TIME_PARAM, device=device, output_path=OUTPUT_IMAGE)\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T21:27:01.941250Z","iopub.execute_input":"2025-04-04T21:27:01.941629Z","iopub.status.idle":"2025-04-05T09:08:25.225111Z","shell.execute_reply.started":"2025-04-04T21:27:01.941598Z","shell.execute_reply":"2025-04-05T09:08:25.224022Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 204MB/s] \n                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [1/26] completed in 1619.27 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_1.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [2/26] completed in 1618.15 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_2.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [3/26] completed in 1618.17 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_3.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [4/26] completed in 1618.16 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_4.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [5/26] completed in 1618.11 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_5.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [6/26] completed in 1618.23 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_6.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [7/26] completed in 1618.19 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_7.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [8/26] completed in 1618.12 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_8.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [9/26] completed in 1618.10 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_9.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [10/26] completed in 1618.24 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_10.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [11/26] completed in 1618.26 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_11.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [12/26] completed in 1618.07 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_12.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [13/26] completed in 1617.87 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_13.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [14/26] completed in 1618.13 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_14.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [15/26] completed in 1618.11 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_15.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [16/26] completed in 1618.21 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_16.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [17/26] completed in 1618.20 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_17.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [18/26] completed in 1618.08 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_18.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [19/26] completed in 1618.22 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_19.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [20/26] completed in 1618.06 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_20.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [21/26] completed in 1618.05 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_21.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [22/26] completed in 1618.00 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_22.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [23/26] completed in 1618.00 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_23.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [24/26] completed in 1618.13 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_24.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [25/26] completed in 1617.96 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_25.pth.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [26/26] completed in 1618.11 seconds. Checkpoint saved to checkpoints/checkpoint_epoch_26.pth.\n","output_type":"stream"}],"execution_count":1}]}